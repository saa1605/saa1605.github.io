<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-10-21T19:48:13+05:30</updated><id>http://localhost:4000/feed.xml</id><title type="html">Saaket Agashe</title><subtitle>An amazing website.</subtitle><author><name>Saaket Agashe</name></author><entry><title type="html">Word2vec</title><link href="http://localhost:4000/Word2Vec/" rel="alternate" type="text/html" title="Word2vec" /><published>2020-10-21T00:00:00+05:30</published><updated>2020-10-21T00:00:00+05:30</updated><id>http://localhost:4000/Word2Vec</id><content type="html" xml:base="http://localhost:4000/Word2Vec/">&lt;h1 id=&quot;word2vec-the-intuition-the-mathematics-and-the-code&quot;&gt;Word2Vec, The intuition, the mathematics and the code&lt;/h1&gt;

&lt;p&gt;Created: Oct 6, 2020 9:09 PM&lt;/p&gt;

&lt;p&gt;Word Embeddings have been around for about 6 years now, so blog posts like this have also been around for the same time. And yet I think its a great idea to right another one because I am a moron. Lets get started.&lt;/p&gt;

&lt;p&gt;I am going to talk about the famous Word2Vec algorithm proposed by Mikolov et al. in their paper titled &lt;strong&gt;&lt;em&gt;Distributed Representations of Words and Phrases and their Compositionality.&lt;/em&gt;&lt;/strong&gt; The paper showed a great example displaying how the representations learned by their model is able to capture the salient features of the english language and semantic understanding using analogies as evidence. The famous example of “King” - “Man” + “Woman” = “Queen” is something that even the people living in the amazon forests have now probably heard. But to tell you the truth, before starting to write this blog post I had not the foggiest of ideas about the mathematical and practical details of this model. But I did promise that I will give you an intuition of what it is so lets start with that&lt;/p&gt;

&lt;h3 id=&quot;the-skip-gram-model&quot;&gt;The Skip-Gram Model&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/posts/word2vec/skip-gram-data-image.png&quot; alt=&quot;skip-gram-data-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I would like to bring your attention to this shamelessly stolen image from Chris McCormick’s &lt;a href=&quot;https://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/&quot;&gt;blog&lt;/a&gt;, which contains an obviously better written article. The text which brings back terrible memories from primary school english classes is our source text. We are interested in looking at a word and then trying to find out the words that are in its proximity. This is because a linguist named John Rupert Firth decided to be snarky and quote &lt;strong&gt;&lt;em&gt;You shall know a word by the company it keeps.&lt;/em&gt;&lt;/strong&gt; Consider the instance where the window cover “quick brown fox jumps over”, here the blue marker lies on “fox” which is the target word, while the words “quick”, “brown”, “jumps” and “over” are known as the context words. Extracting this data is a secret tool that is going to help us later.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/posts/word2vec/skip-gram-data-image.png&quot; alt=&quot;skip-gram-data-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/posts/word2vec/skip-gram-data-image.png&quot; alt=&quot;skip-gram-data-image&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;We are going to talk about Word Embedding using Word2Vec. Developed by Google in 2013-14. Use Neural Networks. (Latent semantic analysis relevant here). (Word Embeddings + Context Embeddings)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Word2Vec&lt;/li&gt;
  &lt;li&gt;CBOW&lt;/li&gt;
  &lt;li&gt;Skip Gram&lt;/li&gt;
  &lt;li&gt;One-Word Learning&lt;/li&gt;
  &lt;li&gt;Input Layer&lt;/li&gt;
  &lt;li&gt;Hidden Layer&lt;/li&gt;
  &lt;li&gt;Output Layer&lt;/li&gt;
  &lt;li&gt;Update&lt;/li&gt;
  &lt;li&gt;CBOW - multiple Words&lt;/li&gt;
  &lt;li&gt;Loss Function&lt;/li&gt;
  &lt;li&gt;What does it learn?&lt;/li&gt;
  &lt;li&gt;Skip-Gram Model&lt;/li&gt;
  &lt;li&gt;Sub-Sampling&lt;/li&gt;
  &lt;li&gt;Negative Sampling&lt;/li&gt;
  &lt;li&gt;Softmax&lt;/li&gt;
  &lt;li&gt;Hierarchical Softmax&lt;/li&gt;
  &lt;li&gt;Limitations&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;goal&quot;&gt;Goal&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Process each word in vocabulary to obtain a respective numeric representation of each word in the vocab&lt;/li&gt;
  &lt;li&gt;Reflect semantic and syntactic similarities&lt;/li&gt;
  &lt;li&gt;Map each of the plurality of words to a respective vector and output a single merged vector that is a combination of the respective vectors.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;context-words-and-central-word&quot;&gt;Context Words and Central Word&lt;/h3&gt;

&lt;p&gt;CBOW: A central word is surrounded by context words. Given the context words, identify the central word&lt;/p&gt;

&lt;p&gt;Skip Gram Model- Given the central word, identify the surrounding words&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/posts/word2vec/skip-gram-data-image.png&quot; alt=&quot;skip-gram-data-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;CBOW Architecture&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/posts/word2vec/skip-gram-data-image.png&quot; alt=&quot;skip-gram-data-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Skip-Gram Architecture&lt;/p&gt;

&lt;h3 id=&quot;one-word-learning&quot;&gt;One Word Learning&lt;/h3&gt;

&lt;p&gt;Add the window animation here&lt;/p&gt;

&lt;h3 id=&quot;bowtie-models&quot;&gt;Bowtie Models&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/posts/word2vec/skip-gram-data-image.png&quot; alt=&quot;skip-gram-data-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A CBOW model with only one word as input. The layers are fully connected. Here &lt;em&gt;W&lt;/em&gt; and &lt;em&gt;W’&lt;/em&gt; are learned&lt;/p&gt;

&lt;p&gt;Mention One Hot Encoding&lt;/p&gt;

&lt;h3 id=&quot;forward-pass&quot;&gt;Forward Pass&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Hidden Layer&lt;/strong&gt;&lt;/p&gt;

\[h = W^TX\]

\[u_j = v_{w_{j}}^{'T}v_{w_{I}}\]

&lt;p&gt;&lt;strong&gt;Output Layer&lt;/strong&gt;&lt;/p&gt;

\[p(w_j|w_I) = y_j\]

&lt;p&gt;where $y_j$ is the output of the $j^{th}$ unit in output layer&lt;/p&gt;

\[y_j = \frac{e^{u_j}}{\sum_{i'=1}^{V}e^{u_{j'}}}\]</content><author><name>Saaket Agashe</name></author><summary type="html">Word2Vec, The intuition, the mathematics and the code</summary></entry><entry><title type="html">Word2vec_g</title><link href="http://localhost:4000/Word2Vec_g/" rel="alternate" type="text/html" title="Word2vec_g" /><published>2020-10-21T00:00:00+05:30</published><updated>2020-10-21T00:00:00+05:30</updated><id>http://localhost:4000/Word2Vec_g</id><content type="html" xml:base="http://localhost:4000/Word2Vec_g/">&lt;h1 id=&quot;word2vec-the-intuition-the-mathematics-and-the-code&quot;&gt;Word2Vec, The intuition, the mathematics and the code&lt;/h1&gt;

&lt;p&gt;Created: Oct 6, 2020 9:09 PM&lt;/p&gt;

&lt;p&gt;Word Embeddings have been around for about 6 years now, so blog posts
like this have also been around for the same time. And yet I think its a
great idea to right another one because I am a moron. Lets get started.&lt;/p&gt;

&lt;p&gt;I am going to talk about the famous Word2Vec algorithm proposed by
Mikolov et al. in their paper titled &lt;strong&gt;&lt;em&gt;Distributed Representations of
Words and Phrases and their Compositionality.&lt;/em&gt;&lt;/strong&gt; The paper showed a
great example displaying how the representations learned by their model
is able to capture the salient features of the english language and
semantic understanding using analogies as evidence. The famous example
of “King” - “Man” + “Woman” = “Queen” is something that even the people
living in the amazon forests have now probably heard. But to tell you
the truth, before starting to write this blog post I had not the
foggiest of ideas about the mathematical and practical details of this
model. But I did promise that I will give you an intuition of what it is
so lets start with that&lt;/p&gt;

&lt;h3 id=&quot;the-skip-gram-model&quot;&gt;The Skip-Gram Model&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/posts/word2vec/skip-gram-data-image.png&quot; alt=&quot;skip-gram-data-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I would like to bring your attention to this shamelessly stolen image
from Chris McCormick’s
&lt;a href=&quot;https://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/&quot;&gt;blog&lt;/a&gt;,
which contains an obviously better written article. The text which
brings back terrible memories from primary school english classes is our
source text. We are interested in looking at a word and then trying to
find out the words that are in its proximity. This is because a linguist
named John Rupert Firth decided to be snarky and quote &lt;strong&gt;&lt;em&gt;You shall know
a word by the company it keeps.&lt;/em&gt;&lt;/strong&gt; Consider the instance where the
window cover “quick brown fox jumps over”, here the blue marker lies on
“fox” which is the target word, while the words “quick”, “brown”,
“jumps” and “over” are known as the context words. Extracting this data
is a secret tool that is going to help us later.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/posts/word2vec/skip-gram-data-image.png&quot; alt=&quot;skip-gram-data-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/posts/word2vec/skip-gram-data-image.png&quot; alt=&quot;skip-gram-data-image&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;We are going to talk about Word Embedding using Word2Vec. Developed by
Google in 2013-14. Use Neural Networks. (Latent semantic analysis
relevant here). (Word Embeddings + Context Embeddings)&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Word2Vec&lt;/li&gt;
  &lt;li&gt;CBOW&lt;/li&gt;
  &lt;li&gt;Skip Gram&lt;/li&gt;
  &lt;li&gt;One-Word Learning&lt;/li&gt;
  &lt;li&gt;Input Layer&lt;/li&gt;
  &lt;li&gt;Hidden Layer&lt;/li&gt;
  &lt;li&gt;Output Layer&lt;/li&gt;
  &lt;li&gt;Update&lt;/li&gt;
  &lt;li&gt;CBOW - multiple Words&lt;/li&gt;
  &lt;li&gt;Loss Function&lt;/li&gt;
  &lt;li&gt;What does it learn?&lt;/li&gt;
  &lt;li&gt;Skip-Gram Model&lt;/li&gt;
  &lt;li&gt;Sub-Sampling&lt;/li&gt;
  &lt;li&gt;Negative Sampling&lt;/li&gt;
  &lt;li&gt;Softmax&lt;/li&gt;
  &lt;li&gt;Hierarchical Softmax&lt;/li&gt;
  &lt;li&gt;Limitations&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;goal&quot;&gt;Goal&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Process each word in vocabulary to obtain a respective numeric
representation of each word in the vocab&lt;/li&gt;
  &lt;li&gt;Reflect semantic and syntactic similarities&lt;/li&gt;
  &lt;li&gt;Map each of the plurality of words to a respective vector and output
a single merged vector that is a combination of the respective
vectors.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;context-words-and-central-word&quot;&gt;Context Words and Central Word&lt;/h3&gt;

&lt;p&gt;CBOW: A central word is surrounded by context words. Given the context
words, identify the central word&lt;/p&gt;

&lt;p&gt;Skip Gram Model- Given the central word, identify the surrounding words&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/posts/word2vec/skip-gram-data-image.png&quot; alt=&quot;skip-gram-data-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;CBOW Architecture&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/posts/word2vec/skip-gram-data-image.png&quot; alt=&quot;skip-gram-data-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Skip-Gram Architecture&lt;/p&gt;

&lt;h3 id=&quot;one-word-learning&quot;&gt;One Word Learning&lt;/h3&gt;

&lt;p&gt;Add the window animation here&lt;/p&gt;

&lt;h3 id=&quot;bowtie-models&quot;&gt;Bowtie Models&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/posts/word2vec/skip-gram-data-image.png&quot; alt=&quot;skip-gram-data-image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A CBOW model with only one word as input. The layers are fully
connected. Here &lt;em&gt;W&lt;/em&gt; and &lt;em&gt;W’&lt;/em&gt; are learned&lt;/p&gt;

&lt;p&gt;Mention One Hot Encoding&lt;/p&gt;

&lt;h3 id=&quot;forward-pass&quot;&gt;Forward Pass&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Hidden Layer&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;h&lt;/em&gt; = &lt;em&gt;W&lt;/em&gt;&lt;sup&gt;&lt;em&gt;T&lt;/em&gt;&lt;/sup&gt;&lt;em&gt;X&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;u&lt;/em&gt;&lt;sub&gt;&lt;em&gt;j&lt;/em&gt;&lt;/sub&gt; = &lt;em&gt;v&lt;/em&gt;&lt;sub&gt;&lt;em&gt;w&lt;/em&gt;&lt;sub&gt;&lt;em&gt;j&lt;/em&gt;&lt;/sub&gt;&lt;/sub&gt;&lt;sup&gt;′&lt;em&gt;T&lt;/em&gt;&lt;/sup&gt;&lt;em&gt;v&lt;/em&gt;&lt;sub&gt;&lt;em&gt;w&lt;/em&gt;&lt;sub&gt;&lt;em&gt;I&lt;/em&gt;&lt;/sub&gt;&lt;/sub&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Output Layer&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;p&lt;/em&gt;(&lt;em&gt;w&lt;/em&gt;&lt;sub&gt;&lt;em&gt;j&lt;/em&gt;&lt;/sub&gt;|&lt;em&gt;w&lt;/em&gt;&lt;sub&gt;&lt;em&gt;I&lt;/em&gt;&lt;/sub&gt;) = &lt;em&gt;y&lt;/em&gt;&lt;sub&gt;&lt;em&gt;j&lt;/em&gt;&lt;/sub&gt;&lt;/p&gt;

&lt;p&gt;where &lt;em&gt;y&lt;/em&gt;&lt;sub&gt;&lt;em&gt;j&lt;/em&gt;&lt;/sub&gt; is the output of the &lt;em&gt;j&lt;/em&gt;&lt;sup&gt;&lt;em&gt;t**h&lt;/em&gt;&lt;/sup&gt; unit
in output layer&lt;/p&gt;

\[y\_j = \\frac{e^{u\_j}}{\\sum\_{i'=1}^{V}e^{u\_{j'}}}\]</content><author><name>Saaket Agashe</name></author><summary type="html">Word2Vec, The intuition, the mathematics and the code</summary></entry></feed>
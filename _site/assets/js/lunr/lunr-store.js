var store = [{
        "title": "AVITRA",
        "excerpt":"Introduction   The tremendous development in the field of robotics has today paved the way for working on the implementation of tasks without human intervention. The development of industry standards such as Robot Operating Systems (ROS) and extensive access to resources has made it possible for budding engineers like us to contribute to the sector of Intelligent Systems. The automation sector today is being powered by autonomous systems and multi-degree of freedom robots are playing a very important role in different applications of automation. They are providing much more accuracy and safety in carrying out critical tasks as compared to the manual work done by humans. This project describes the design and implementation of a 6 DOF robotic arm capable of performing industrial tasks such as pick and place of objects. The design aims to provide fine manipulation in performing pick and place tasks without any human intervention. By integrating the manipulator with a mobile base, an autonomous robot system capable of navigating unstructured terrains and manipulating objects for various applications like drawing, soldering, pick and place, welding etc. can be built. With the help of a depth camera, perception is implemented to enhance the detection techniques thus helping in distinguishing objects and obstacles in applications like search and rescue operation. These perception techniques when collaborated with mobile manipulation can help reaching places and handling objects which would otherwise prove deleterious to humans. With a highly flexible multi-DOF robotic arm, a flexible end effector capable of holding objects of different shapes is also a requirement for manipulation of objects in an unknown environment. Adding a gripper possessing a high holding force helps in grasping objects of different shapes and performing tasks like drawing, soldering etc. where grasping of the object needs to be precise in order to carry out a task. Thus, this project aims to build and implement a robot system with all these above mentioned capabilities.  ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/portfolio/avitra/",
        "teaser": "http://localhost:4000/assets/images/avitra-manipulator.jpg"
      },{
        "title": "Word2vec",
        "excerpt":"Word2Vec, The intuition, the mathematics and the code   Created: Oct 6, 2020 9:09 PM   Word Embeddings have been around for about 6 years now, so blog posts like this have also been around for the same time. And yet I think its a great idea to right another one because I am a moron. Lets get started.   I am going to talk about the famous Word2Vec algorithm proposed by Mikolov et al. in their paper titled Distributed Representations of Words and Phrases and their Compositionality. The paper showed a great example displaying how the representations learned by their model is able to capture the salient features of the english language and semantic understanding using analogies as evidence. The famous example of “King” - “Man” + “Woman” = “Queen” is something that even the people living in the amazon forests have now probably heard. But to tell you the truth, before starting to write this blog post I had not the foggiest of ideas about the mathematical and practical details of this model. But I did promise that I will give you an intuition of what it is so lets start with that   The Skip-Gram Model      I would like to bring your attention to this shamelessly stolen image from Chris McCormick’s blog, which contains an obviously better written article. The text which brings back terrible memories from primary school english classes is our source text. We are interested in looking at a word and then trying to find out the words that are in its proximity. This is because a linguist named John Rupert Firth decided to be snarky and quote You shall know a word by the company it keeps. Consider the instance where the window cover “quick brown fox jumps over”, here the blue marker lies on “fox” which is the target word, while the words “quick”, “brown”, “jumps” and “over” are known as the context words. Extracting this data is a secret tool that is going to help us later.           We are going to talk about Word Embedding using Word2Vec. Developed by Google in 2013-14. Use Neural Networks. (Latent semantic analysis relevant here). (Word Embeddings + Context Embeddings)      Word2Vec   CBOW   Skip Gram   One-Word Learning   Input Layer   Hidden Layer   Output Layer   Update   CBOW - multiple Words   Loss Function   What does it learn?   Skip-Gram Model   Sub-Sampling   Negative Sampling   Softmax   Hierarchical Softmax   Limitations   Goal      Process each word in vocabulary to obtain a respective numeric representation of each word in the vocab   Reflect semantic and syntactic similarities   Map each of the plurality of words to a respective vector and output a single merged vector that is a combination of the respective vectors.   Context Words and Central Word   CBOW: A central word is surrounded by context words. Given the context words, identify the central word   Skip Gram Model- Given the central word, identify the surrounding words      CBOW Architecture      Skip-Gram Architecture   One Word Learning   Add the window animation here   Bowtie Models      A CBOW model with only one word as input. The layers are fully connected. Here W and W’ are learned   Mention One Hot Encoding   Forward Pass   Hidden Layer   \\[h = W^TX\\]  \\[u_j = v_{w_{j}}^{'T}v_{w_{I}}\\]  Output Layer   \\[p(w_j|w_I) = y_j\\]  where $y_j$ is the output of the $j^{th}$ unit in output layer   \\[y_j = \\frac{e^{u_j}}{\\sum_{i'=1}^{V}e^{u_{j'}}}\\] ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/Word2Vec/",
        "teaser": null
      },{
        "title": "Word2vec_g",
        "excerpt":"Word2Vec, The intuition, the mathematics and the code   Created: Oct 6, 2020 9:09 PM   Word Embeddings have been around for about 6 years now, so blog posts like this have also been around for the same time. And yet I think its a great idea to right another one because I am a moron. Lets get started.   I am going to talk about the famous Word2Vec algorithm proposed by Mikolov et al. in their paper titled Distributed Representations of Words and Phrases and their Compositionality. The paper showed a great example displaying how the representations learned by their model is able to capture the salient features of the english language and semantic understanding using analogies as evidence. The famous example of “King” - “Man” + “Woman” = “Queen” is something that even the people living in the amazon forests have now probably heard. But to tell you the truth, before starting to write this blog post I had not the foggiest of ideas about the mathematical and practical details of this model. But I did promise that I will give you an intuition of what it is so lets start with that   The Skip-Gram Model      I would like to bring your attention to this shamelessly stolen image from Chris McCormick’s blog, which contains an obviously better written article. The text which brings back terrible memories from primary school english classes is our source text. We are interested in looking at a word and then trying to find out the words that are in its proximity. This is because a linguist named John Rupert Firth decided to be snarky and quote You shall know a word by the company it keeps. Consider the instance where the window cover “quick brown fox jumps over”, here the blue marker lies on “fox” which is the target word, while the words “quick”, “brown”, “jumps” and “over” are known as the context words. Extracting this data is a secret tool that is going to help us later.           We are going to talk about Word Embedding using Word2Vec. Developed by Google in 2013-14. Use Neural Networks. (Latent semantic analysis relevant here). (Word Embeddings + Context Embeddings)      Word2Vec   CBOW   Skip Gram   One-Word Learning   Input Layer   Hidden Layer   Output Layer   Update   CBOW - multiple Words   Loss Function   What does it learn?   Skip-Gram Model   Sub-Sampling   Negative Sampling   Softmax   Hierarchical Softmax   Limitations   Goal      Process each word in vocabulary to obtain a respective numeric representation of each word in the vocab   Reflect semantic and syntactic similarities   Map each of the plurality of words to a respective vector and output a single merged vector that is a combination of the respective vectors.   Context Words and Central Word   CBOW: A central word is surrounded by context words. Given the context words, identify the central word   Skip Gram Model- Given the central word, identify the surrounding words      CBOW Architecture      Skip-Gram Architecture   One Word Learning   Add the window animation here   Bowtie Models      A CBOW model with only one word as input. The layers are fully connected. Here W and W’ are learned   Mention One Hot Encoding   Forward Pass   Hidden Layer   h = WTX   uj = vwj′TvwI   Output Layer   p(wj|wI) = yj   where yj is the output of the jt**h unit in output layer   \\[y\\_j = \\\\frac{e^{u\\_j}}{\\\\sum\\_{i'=1}^{V}e^{u\\_{j'}}}\\] ","categories": [],
        "tags": [],
        "url": "http://localhost:4000/Word2Vec_g/",
        "teaser": null
      }]
